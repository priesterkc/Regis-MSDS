{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment - MSDS 682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "The dataset used in this assignment was collected from [r/datascience on Reddit](https://www.reddit.com/r/datascience/) for the time period of April 8-14, 2019, using the Pushshift API. The data contains information for each thread posted in the subreddit such as the date the post was created, the author, the title of the post, the flair (topic category) of the post, and number of comments about the post. I will explore frequently occuring single-term words, bigrams, and trigrams, and also use predictive modeling to determine the flair category of a post, and use topic modeling to find distinct subject groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "en_stopwords = stopwords.words('english')  \n",
    "stopwords = set(en_stopwords)  #stopwords has split contractions in it\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "\n",
    "This section of the notebook loads in the data gathered from r/datascience on Reddit. The dataset has 236 rows and 9 columns. I then modified the `cleanCorpus` function from previous assignments and ran it on the `title` column of the dataframe, which contains the text from each thread post title in the subreddit. Once processed, I checked for thread title duplication and removed rows in which a user posted the same thread twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data collected from Reddit stored as csv file\n",
    "ds_df = pd.read_csv(\"datafrom_r_datascience.csv\")\n",
    "ds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bamzb9</td>\n",
       "      <td>1554682018</td>\n",
       "      <td>bbbobbyb</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Is it hard to get into a Masters in Data Scien...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bamzb9/is_it_hard_to_g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ban361</td>\n",
       "      <td>1554682674</td>\n",
       "      <td>bobbyb2222</td>\n",
       "      <td>False</td>\n",
       "      <td>Education</td>\n",
       "      <td>Is it hard to get into a Masters in Data Scien...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/ban361/is_it_hard_to_g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banj60</td>\n",
       "      <td>1554685531</td>\n",
       "      <td>rdy1107</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just finished an academic thesis on technical ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/banj60/just_finished_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>banxus</td>\n",
       "      <td>1554688162</td>\n",
       "      <td>TwoToneDonut</td>\n",
       "      <td>False</td>\n",
       "      <td>Education</td>\n",
       "      <td>EdX Microsoft Professional Program - Data Scie...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/banxus/edx_microsoft_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baod6v</td>\n",
       "      <td>1554690889</td>\n",
       "      <td>CharlesPolley</td>\n",
       "      <td>False</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>10 Data Structure, Algorithms, and SQL Courses...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/datascience/comments/baod6v/10_data_structu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  created_utc         author  can_mod_post link_flair_text  \\\n",
       "0  bamzb9   1554682018       bbbobbyb         False             NaN   \n",
       "1  ban361   1554682674     bobbyb2222         False       Education   \n",
       "2  banj60   1554685531        rdy1107         False             NaN   \n",
       "3  banxus   1554688162   TwoToneDonut         False       Education   \n",
       "4  baod6v   1554690889  CharlesPolley         False      Discussion   \n",
       "\n",
       "                                               title  num_comments  score  \\\n",
       "0  Is it hard to get into a Masters in Data Scien...             1      1   \n",
       "1  Is it hard to get into a Masters in Data Scien...             2      1   \n",
       "2  Just finished an academic thesis on technical ...             1      1   \n",
       "3  EdX Microsoft Professional Program - Data Scie...             0      1   \n",
       "4  10 Data Structure, Algorithms, and SQL Courses...             1      0   \n",
       "\n",
       "                                           permalink  \n",
       "0  /r/datascience/comments/bamzb9/is_it_hard_to_g...  \n",
       "1  /r/datascience/comments/ban361/is_it_hard_to_g...  \n",
       "2  /r/datascience/comments/banj60/just_finished_a...  \n",
       "3  /r/datascience/comments/banxus/edx_microsoft_p...  \n",
       "4  /r/datascience/comments/baod6v/10_data_structu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify first 5 rows of data\n",
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will take in each row of a pandas dataframe column (corpus) that contains each document (title text) as a string after making the text lowercase, \n",
    "removing punctuation/digits/special characters, converting into a spaCy object (tokenizing), removing stopwords, and rejoining as a string.\n",
    "The final string product is then returned to the dataframe.\n",
    "\"\"\"\n",
    "\n",
    "#function to lowercase, remove punctuation/digits, tokenize (split into words), and remove stopwords\n",
    "def cleanCorpus(corpus):\n",
    "\n",
    "    #remove punctuation, digits, and non-standard single quotes/apostrophes\n",
    "    rmv = str.maketrans({key: None for key in string.punctuation + string.digits + \"‘’\"})\n",
    "\n",
    "    #make string lowercase                    \n",
    "    lower_str = corpus.translate(rmv).lower()\n",
    "\n",
    "    #make string into a spaCy object\n",
    "    nlpdoc = nlp(lower_str)\n",
    "\n",
    "    #holds tokens that are not stopwords\n",
    "    clean_wordls = []\n",
    "\n",
    "    #each token in the spaCy object\n",
    "    for token in nlpdoc:\n",
    "        \n",
    "        #get the text for the token\n",
    "        tknwd = token.text\n",
    "        \n",
    "        #list of exception words\n",
    "        exceptions = ['but', 'nor', 'not']\n",
    "        \n",
    "        #check to see if the token is a stopword\n",
    "        if tknwd in stopwords:\n",
    "            \n",
    "            #check to see if token is an exception word\n",
    "            #if True, then add to the clean words list\n",
    "            #if False, skip the word; don't add to list\n",
    "            if tknwd in exceptions: clean_wordls.append(tknwd)\n",
    "            else: pass\n",
    "        \n",
    "        #if word is not a stopword, add to clean words list\n",
    "        else:\n",
    "            clean_wordls.append(tknwd)\n",
    "\n",
    "    #join list of cleaned words together into one string\n",
    "    clean_str = \" \".join(clean_wordls)  \n",
    "    \n",
    "    #returned output is an item from corpus as a string\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a copy of original dataframe\n",
    "clean_ds_df = ds_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run the cleanCorpus function on the thread title text\n",
    "clean_ds_df['title'] = clean_ds_df['title'].apply(cleanCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bamzb9</td>\n",
       "      <td>1554682018</td>\n",
       "      <td>bbbobbyb</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hard get masters data science program</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bamzb9/is_it_hard_to_g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ban361</td>\n",
       "      <td>1554682674</td>\n",
       "      <td>bobbyb2222</td>\n",
       "      <td>False</td>\n",
       "      <td>Education</td>\n",
       "      <td>hard get masters data science program</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/ban361/is_it_hard_to_g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banj60</td>\n",
       "      <td>1554685531</td>\n",
       "      <td>rdy1107</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>finished academic thesis technical analysis wr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/banj60/just_finished_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>banxus</td>\n",
       "      <td>1554688162</td>\n",
       "      <td>TwoToneDonut</td>\n",
       "      <td>False</td>\n",
       "      <td>Education</td>\n",
       "      <td>edx microsoft professional program   data scie...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/banxus/edx_microsoft_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baod6v</td>\n",
       "      <td>1554690889</td>\n",
       "      <td>CharlesPolley</td>\n",
       "      <td>False</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>data structure algorithms sql courses crack ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/datascience/comments/baod6v/10_data_structu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  created_utc         author  can_mod_post link_flair_text  \\\n",
       "0  bamzb9   1554682018       bbbobbyb         False             NaN   \n",
       "1  ban361   1554682674     bobbyb2222         False       Education   \n",
       "2  banj60   1554685531        rdy1107         False             NaN   \n",
       "3  banxus   1554688162   TwoToneDonut         False       Education   \n",
       "4  baod6v   1554690889  CharlesPolley         False      Discussion   \n",
       "\n",
       "                                               title  num_comments  score  \\\n",
       "0              hard get masters data science program             1      1   \n",
       "1              hard get masters data science program             2      1   \n",
       "2  finished academic thesis technical analysis wr...             1      1   \n",
       "3  edx microsoft professional program   data scie...             0      1   \n",
       "4    data structure algorithms sql courses crack ...             1      0   \n",
       "\n",
       "                                           permalink  \n",
       "0  /r/datascience/comments/bamzb9/is_it_hard_to_g...  \n",
       "1  /r/datascience/comments/ban361/is_it_hard_to_g...  \n",
       "2  /r/datascience/comments/banj60/just_finished_a...  \n",
       "3  /r/datascience/comments/banxus/edx_microsoft_p...  \n",
       "4  /r/datascience/comments/baod6v/10_data_structu...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify cleaned text in new dataframe\n",
    "clean_ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ban361</td>\n",
       "      <td>1554682674</td>\n",
       "      <td>bobbyb2222</td>\n",
       "      <td>False</td>\n",
       "      <td>Education</td>\n",
       "      <td>hard get masters data science program</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/ban361/is_it_hard_to_g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>barv7c</td>\n",
       "      <td>1554717195</td>\n",
       "      <td>plexex</td>\n",
       "      <td>False</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>best tool learn data science</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/barv7c/the_best_tool_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bauj4r</td>\n",
       "      <td>1554734042</td>\n",
       "      <td>arnauda9</td>\n",
       "      <td>False</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>apache airflow distributes jobs celery workers</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>/r/datascience/comments/bauj4r/how_apache_airf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>baw5jz</td>\n",
       "      <td>1554742284</td>\n",
       "      <td>arnauda9</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apache airflow distributes jobs celery workers</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/baw5jz/how_apache_airf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>bb5le5</td>\n",
       "      <td>1554800041</td>\n",
       "      <td>syslynx</td>\n",
       "      <td>False</td>\n",
       "      <td>Projects</td>\n",
       "      <td>guys hosting first presentation care help</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bb5le5/guys_im_hosting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>bb8a1k</td>\n",
       "      <td>1554818419</td>\n",
       "      <td>jeetugalav</td>\n",
       "      <td>False</td>\n",
       "      <td>Projects</td>\n",
       "      <td>state data analytics</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bb8a1k/state_of_data_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>bb9re3</td>\n",
       "      <td>1554826110</td>\n",
       "      <td>awkwardable</td>\n",
       "      <td>False</td>\n",
       "      <td>Career</td>\n",
       "      <td>data analyst intern vs data science intern</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/datascience/comments/bb9re3/data_analyst_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>bbegxh</td>\n",
       "      <td>1554849706</td>\n",
       "      <td>generalizederror</td>\n",
       "      <td>False</td>\n",
       "      <td>Job Search</td>\n",
       "      <td>machine learning research job interview experi...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>/r/datascience/comments/bbegxh/d_my_machine_le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>bbtnad</td>\n",
       "      <td>1554942643</td>\n",
       "      <td>Chaostorrent48</td>\n",
       "      <td>False</td>\n",
       "      <td>Education</td>\n",
       "      <td>given   months free time data science</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bbtnad/what_to_do_give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>bbza4h</td>\n",
       "      <td>1554985722</td>\n",
       "      <td>multiks2200</td>\n",
       "      <td>False</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>data scientist statistical tests one need know</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bbza4h/as_a_data_scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>bcjvp8</td>\n",
       "      <td>1555109013</td>\n",
       "      <td>treguess</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stanford presents deep learning digitizing hum...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bcjvp8/stanford_presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>bcp1ot</td>\n",
       "      <td>1555150009</td>\n",
       "      <td>anwar_atar</td>\n",
       "      <td>False</td>\n",
       "      <td>Education</td>\n",
       "      <td>free webinar hr analytics</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/datascience/comments/bcp1ot/free_webinar_on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  created_utc            author  can_mod_post link_flair_text  \\\n",
       "1    ban361   1554682674        bobbyb2222         False       Education   \n",
       "10   barv7c   1554717195            plexex         False      Discussion   \n",
       "21   bauj4r   1554734042          arnauda9         False      Discussion   \n",
       "30   baw5jz   1554742284          arnauda9         False             NaN   \n",
       "55   bb5le5   1554800041           syslynx         False        Projects   \n",
       "65   bb8a1k   1554818419        jeetugalav         False        Projects   \n",
       "72   bb9re3   1554826110       awkwardable         False          Career   \n",
       "81   bbegxh   1554849706  generalizederror         False      Job Search   \n",
       "133  bbtnad   1554942643    Chaostorrent48         False       Education   \n",
       "149  bbza4h   1554985722       multiks2200         False      Discussion   \n",
       "208  bcjvp8   1555109013          treguess         False             NaN   \n",
       "221  bcp1ot   1555150009        anwar_atar         False       Education   \n",
       "\n",
       "                                                 title  num_comments  score  \\\n",
       "1                hard get masters data science program             2      1   \n",
       "10                        best tool learn data science             2      1   \n",
       "21      apache airflow distributes jobs celery workers             0      2   \n",
       "30      apache airflow distributes jobs celery workers             0      1   \n",
       "55           guys hosting first presentation care help             2      1   \n",
       "65                                state data analytics             1      1   \n",
       "72          data analyst intern vs data science intern             2      0   \n",
       "81   machine learning research job interview experi...             0      2   \n",
       "133              given   months free time data science             2      1   \n",
       "149     data scientist statistical tests one need know             3      1   \n",
       "208  stanford presents deep learning digitizing hum...             0      1   \n",
       "221                          free webinar hr analytics             0      1   \n",
       "\n",
       "                                             permalink  \n",
       "1    /r/datascience/comments/ban361/is_it_hard_to_g...  \n",
       "10   /r/datascience/comments/barv7c/the_best_tool_t...  \n",
       "21   /r/datascience/comments/bauj4r/how_apache_airf...  \n",
       "30   /r/datascience/comments/baw5jz/how_apache_airf...  \n",
       "55   /r/datascience/comments/bb5le5/guys_im_hosting...  \n",
       "65   /r/datascience/comments/bb8a1k/state_of_data_a...  \n",
       "72   /r/datascience/comments/bb9re3/data_analyst_in...  \n",
       "81   /r/datascience/comments/bbegxh/d_my_machine_le...  \n",
       "133  /r/datascience/comments/bbtnad/what_to_do_give...  \n",
       "149  /r/datascience/comments/bbza4h/as_a_data_scien...  \n",
       "208  /r/datascience/comments/bcjvp8/stanford_presen...  \n",
       "221  /r/datascience/comments/bcp1ot/free_webinar_on...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#duplicated thread titles\n",
    "#some users try to post thread more than once to get it noticed\n",
    "clean_ds_df.loc[clean_ds_df['title'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select rows with no title text or author duplication\n",
    "nodupe_ds_df = clean_ds_df.loc[clean_ds_df[['title', 'author']].duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of original dataframe\n",
    "len(clean_ds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of dataframe without duplicates\n",
    "len(nodupe_ds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data\n",
    "\n",
    "This section examines the frequency of words, bigrams, and trigrams in the title text data. First, I converted the `title` column in the dataframe to a list, then joined all the list items as one whole string value. The string was then passed through the NLTK `word_tokenize` function, which returned a list of all the tokens in the dataset. With all the tokens in a single list, I could then get a frequency distribution count and find the top 10 occuring words in the dataset. Then for every thread title, I collected the possible bigrams and trigrams and also calculated their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a list out of the 'title' column\n",
    "#each list item is a thread title\n",
    "title_ls = list(nodupe_ds_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard get masters data science program',\n",
       " 'hard get masters data science program',\n",
       " 'finished academic thesis technical analysis written python would like get eyes   go check code accompanying pdf',\n",
       " 'edx microsoft professional program   data science last class selection suggestions',\n",
       " '  data structure algorithms sql courses crack programming job interview']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first 5 items in list\n",
    "title_ls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put all items together into a whole string\n",
    "title_str = \" \".join(title_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use word_tokenize function\n",
    "#will convert string into a list\n",
    "#each list item is a token(word)\n",
    "all_tknz = word_tokenize(title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 128),\n",
       " ('science', 61),\n",
       " ('scientist', 15),\n",
       " ('job', 14),\n",
       " ('python', 12),\n",
       " ('know', 12),\n",
       " ('best', 10),\n",
       " ('program', 9),\n",
       " ('vs', 9),\n",
       " ('career', 9)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 occuring words in thread titles\n",
    "FreqDist(all_tknz).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create bigrams from each thread title\n",
    "#append all bigrams from entire dataset into a list\n",
    "all_bgs =[]\n",
    "\n",
    "for topic in title_ls:\n",
    "    \n",
    "    title_tknz = word_tokenize(topic)\n",
    "    \n",
    "    for bigram in ngrams(title_tknz, 2):\n",
    "        all_bgs.append(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hard', 'get'),\n",
       " ('get', 'masters'),\n",
       " ('masters', 'data'),\n",
       " ('data', 'science'),\n",
       " ('science', 'program')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of all bigrams in dataset\n",
    "all_bgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('data', 'science'), 59),\n",
       " (('data', 'scientist'), 15),\n",
       " (('data', 'scientists'), 9),\n",
       " (('data', 'analyst'), 6),\n",
       " (('machine', 'learning'), 4),\n",
       " (('science', 'program'), 3),\n",
       " (('would', 'like'), 3),\n",
       " (('online', 'courses'), 3),\n",
       " (('resume', 'review'), 3),\n",
       " (('science', 'career'), 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 occuring words in thread titles\n",
    "FreqDist(all_bgs).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nisha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#create trigrams from each thread title\n",
    "#append all trigrams from entire dataset into a list\n",
    "all_tgs =[]\n",
    "\n",
    "for topic in title_ls:\n",
    "    \n",
    "    title_tknz = word_tokenize(topic)\n",
    "    \n",
    "    for trigram in ngrams(title_tknz, 3):\n",
    "        all_tgs.append(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('data', 'science', 'program'), 3),\n",
       " (('data', 'science', 'career'), 3),\n",
       " (('data', 'science', 'jobs'), 3),\n",
       " (('hard', 'get', 'masters'), 2),\n",
       " (('get', 'masters', 'data'), 2),\n",
       " (('masters', 'data', 'science'), 2),\n",
       " (('best', 'tool', 'learn'), 2),\n",
       " (('tool', 'learn', 'data'), 2),\n",
       " (('learn', 'data', 'science'), 2),\n",
       " (('apache', 'airflow', 'distributes'), 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of all trigrams in dataset\n",
    "FreqDist(all_tgs).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analytics\n",
    "\n",
    "In order to use predictive modeling on the dataset with the `link_flair_text` (flair) as the target, I subsetted the data to only include rows in which the flair was not missing. Flairs are used as an indicator of the type of thread that was created (to differentiate serious discussion threads from upbeat memes) but all thread posts are not required to have a flair. I mapped each flair category to a numerical value to use in the model and then split the data into training and testing sets. Each non-target feature set was converted into a TF-IDF matrix. The training matricies were then used to build a decision tree model and I created another TF-IDF matrix of all the title text rows for finding 5 clusters using the k-means algorithm. After getting the cluster predictions, then I evaluated the model using a silhouette score and analyzed the terms for topic modeling in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drops of data w/o a flair\n",
    "#will use for Decision Tree model\n",
    "flair_df = nodupe_ds_df.loc[nodupe_ds_df['link_flair_text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataframe with only 'link_text_flair\" and 'title' columns\n",
    "flair_df = flair_df[['link_flair_text', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discussion    38\n",
       "Education     37\n",
       "Career        28\n",
       "Projects      24\n",
       "Job Search     7\n",
       "Tooling        7\n",
       "Fun/Trivia     2\n",
       "Meta           1\n",
       "Name: link_flair_text, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of each flair category\n",
    "flair_df['link_flair_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>hard get masters data science program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "      <td>edx microsoft professional program   data scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discussion</td>\n",
       "      <td>data structure algorithms sql courses crack ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Career</td>\n",
       "      <td>working japan tell job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Discussion</td>\n",
       "      <td>python vs r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  link_flair_text                                              title\n",
       "1       Education              hard get masters data science program\n",
       "3       Education  edx microsoft professional program   data scie...\n",
       "4      Discussion    data structure algorithms sql courses crack ...\n",
       "6          Career                             working japan tell job\n",
       "8      Discussion                                        python vs r"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify firts 5 rows in flair dataframe\n",
    "flair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put each distinct flair category name into a list\n",
    "flair_names = list(flair_df['link_flair_text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify this is a list\n",
    "type(flair_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Education'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first item in list\n",
    "flair_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Education',\n",
       " 'Discussion',\n",
       " 'Career',\n",
       " 'Projects',\n",
       " 'Tooling',\n",
       " 'Job Search',\n",
       " 'Meta',\n",
       " 'Fun/Trivia']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all flair category names\n",
    "flair_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use flair name index positions to create a dictionary\n",
    "#dict_key will be the flair name, dict_value is the index position\n",
    "target_val = {}\n",
    "\n",
    "for flair in flair_names:\n",
    "    target_val[flair] = flair_names.index(flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Career': 2,\n",
       " 'Discussion': 1,\n",
       " 'Education': 0,\n",
       " 'Fun/Trivia': 7,\n",
       " 'Job Search': 5,\n",
       " 'Meta': 6,\n",
       " 'Projects': 3,\n",
       " 'Tooling': 4}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full dictionary of flair name(key) and category number(value)\n",
    "target_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create column to hold the flair category numbers\n",
    "#numbers will be used to train the model (since model can't take string values)\n",
    "#dictionary will look for flair name then assign 'target' column the dict_val\n",
    "flair_df['target'] = flair_df['link_flair_text'].map(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>hard get masters data science program</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "      <td>edx microsoft professional program   data scie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discussion</td>\n",
       "      <td>data structure algorithms sql courses crack ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Career</td>\n",
       "      <td>working japan tell job</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Discussion</td>\n",
       "      <td>python vs r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  link_flair_text                                              title  target\n",
       "1       Education              hard get masters data science program       0\n",
       "3       Education  edx microsoft professional program   data scie...       0\n",
       "4      Discussion    data structure algorithms sql courses crack ...       1\n",
       "6          Career                             working japan tell job       2\n",
       "8      Discussion                                        python vs r       1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify new column in dataframe\n",
    "flair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of flair name (string) column\n",
    "#model_df is dataframe that is for the machine learning models\n",
    "model_df = flair_df.drop('link_flair_text', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#variable that only has the 'title' column\n",
    "#will be used for train/test split and to create matrix for k-means\n",
    "X = model_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training set=60%, test set=40%\n",
    "#X is \"title\", y is \"target\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, model_df['target'], random_state=90, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize TF-IDF vectorizer fucntion to a variable\n",
    "tfidf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn text data into a TF-IDF matrix\n",
    "#each column is a \"term\"(word)\n",
    "#each row is a thread title\n",
    "X_train_mtx = tfidf_vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 365)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#86 rows, 365 columns\n",
    "X_train_mtx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize decision tree model\n",
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build the tree model using the training data\n",
    "tree.fit(X_train_mtx, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9883720930232558"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model has a ~99% accuracy score on the training data\n",
    "tree.score(X_train_mtx, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn the test data into a TF-IDF matrix\n",
    "X_test_mtx = tfidf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feed the test data into the model\n",
    "#generate predictive output into variable \"y_predict\"\n",
    "y_predict = tree.predict(X_test_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1724137931034483"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model had a ~17% accuracy score on the test data\n",
    "tree.score(X_test_mtx, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nisha\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Nisha\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.36      0.26        14\n",
      "           1       0.12      0.21      0.15        14\n",
      "           2       0.67      0.15      0.25        13\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.17      0.17      0.17        58\n",
      "   macro avg       0.14      0.10      0.09        58\n",
      "weighted avg       0.23      0.17      0.16        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictive scores from decision tree model by category\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14\n",
       "0    14\n",
       "2    13\n",
       "3    10\n",
       "4     5\n",
       "5     2\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify number of each category in test set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24\n",
       "0    23\n",
       "2    15\n",
       "3    14\n",
       "5     5\n",
       "7     2\n",
       "4     2\n",
       "6     1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify number of each category in training set\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize the TF-IDF vectorizer\n",
    "tfidf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform the entire thread title dataset into a TF-IDF matrix (unsupervised learning)\n",
    "X_mtx = tfidf_vec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the KMeans algorithm into a variable\n",
    "#initialized with 10 clusters and randomize the data\n",
    "kmeans5 = KMeans(n_clusters=5, random_state=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=90, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build the model with the data\n",
    "kmeans5.fit(X_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict which clusters the tweets belong to\n",
    "cluster_pred = kmeans5.predict(X_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 4, 4, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify first 5 data points' cluster assignments\n",
    "cluster_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 5 clusters, the average silhouette score is: 0.014429950780745184\n"
     ]
    }
   ],
   "source": [
    "#get the silhouette score for the model    \n",
    "sil_avg = silhouette_score(X_mtx, cluster_pred)\n",
    "\n",
    "print(f\"For 5 clusters, the average silhouette score is: {sil_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 10 words in Cluster 0:\n",
      "data\n",
      "science\n",
      "program\n",
      "best\n",
      "learn\n",
      "vs\n",
      "project\n",
      "hard\n",
      "tool\n",
      "school\n",
      "\n",
      "\n",
      "The Top 10 words in Cluster 1:\n",
      "python\n",
      "vs\n",
      "r\n",
      "decision\n",
      "data\n",
      "web\n",
      "scraping\n",
      "using\n",
      "opensource\n",
      "commercial\n",
      "\n",
      "\n",
      "The Top 10 words in Cluster 2:\n",
      "data\n",
      "best\n",
      "scientist\n",
      "help\n",
      "jobs\n",
      "looking\n",
      "actually\n",
      "scientists\n",
      "first\n",
      "analysis\n",
      "\n",
      "\n",
      "The Top 10 words in Cluster 3:\n",
      "free\n",
      "data\n",
      "analytics\n",
      "science\n",
      "given\n",
      "months\n",
      "time\n",
      "webinar\n",
      "hr\n",
      "new\n",
      "\n",
      "\n",
      "The Top 10 words in Cluster 4:\n",
      "data\n",
      "job\n",
      "learning\n",
      "interview\n",
      "machine\n",
      "courses\n",
      "science\n",
      "ds\n",
      "research\n",
      "structure\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the top 10 words for each cluster from the KMeans model (5 clusters)\n",
    "\n",
    "k5cluster_labels = list(set(cluster_pred))\n",
    "\n",
    "for cluster_label in k5cluster_labels:\n",
    "    \n",
    "    cluster_tweets = np.array(X)[cluster_pred == cluster_label]\n",
    "    \n",
    "    top10 = FreqDist(word_tokenize(' '.join(cluster_tweets))).most_common(10)\n",
    "    \n",
    "    print(f\"The Top 10 words in Cluster {cluster_label}:\")\n",
    "    \n",
    "    for word in top10:\n",
    "        print(word[0])\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "r/datascience is a decently active subreddit and even though the dataset was a bit small having only collected a week's worth of posts, I was still able to get some insight that showed distinctive trends. The majority of the posts are labeled with the flairs \"Discussion\", \"Education\", \"Career\", and \"Projects\". Naturally because of the subreddit's name, the terms \"data\" and \"science\" are the most common words in the thread titles but other noteworthy words such as \"scientist\", \"job\", \"python\", and \"career\" show that many posts tend to be focused on the job position and tool used in data science, rather than general discussions about the field. This is also further reflected in the bigram and trigram output.\n",
    "\n",
    "The decision tree model did not do very well on the dataset to predict the flair category based on the TF-IDF matrix. This is most likely because there were not a lot of data points for the model to learn from, both in the data as a whole and for each particular flair. Flair such as 'Meta' and 'Fun/Trivia' had too few examples for the model to learn patterns from, so it was not surprising that overall the model did not perform well. However, \"Discussion\", \"Education\", and \"Career\" seemed to be distinct enough for the model to recognize some patterns in the data, especially with its high precision score for the \"Career\" flair.\n",
    "\n",
    "From the clusters predicted in the k-means clustering model, clusters 0-2 seemed to have more cohesive themes than clusters 3 and 4. Cluster 0 seems to represent questions asked in the subreddit about which data science education program are best for them (the user asking), Cluster 1 reflects the comparision of the python and R languages (users ask quite a bit about which one they should learn) as well as other tools related to working on a personal portfolio project. Cluster 2 gets slightly murky but I think that it reveals a topic related to people looking for their first data scientist jobs (or possibly even asking about what to expect in their first data scientist job). The terms \"jobs\" and \"interview\" also show up in Cluster 4, which makes it look career/job related but words such as \"machine\", \"learning\", \"courses\", and \"research\" seem to indicate that there could be a mixture of topics in this cluster and that we possible need more clusters to make better distinctive topic groups (reflected in the low silhouette score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Loot, R. (2018, October 28). Using Pushshift's API to extract Reddit Submissions. Retrieved from https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
