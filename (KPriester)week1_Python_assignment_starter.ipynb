{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 assignment\n",
    "The assignment is to fill in the blanks, and get the text cleaning function to work.  Here is an example of a function that takes in a string, and returns the lowercased version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_lowercase(a_string):\n",
    "    \"\"\"\n",
    "    Takes in a string and returns the lowercased version of it.\n",
    "    \"\"\"\n",
    "    lowercased_string = a_string.lower()\n",
    "    \n",
    "    return lowercased_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is a string with some capitals.\n"
     ]
    }
   ],
   "source": [
    "test_string = 'Here is a String with SOME Capitals.'\n",
    "lowered_string = make_lowercase(test_string)\n",
    "print(lowered_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `return` part is what actually gives you back the lowercased version of the string.  If you don't have the `return` in there, the function will run, but it won't give you anything back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lists\n",
    "One of the fundamental data types in Python is a list.  This is a data type which consists of things (could be numbers, strings, etc) within square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "test_list = [1, 3, 5, 8]\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists have a number of built-in methods: https://docs.python.org/3/tutorial/datastructures.html\n",
    "But here we will only use the .append() method, which adds to the end of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 8, 100]\n"
     ]
    }
   ],
   "source": [
    "test_list.append(100)\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List comprehensions\n",
    "There are some list comprehensions in the code below.  This is a Python trick which can condense your code.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 36]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a for loop that makes a list of the numbers 2, 4, 6, squared (2^2, etc)\n",
    "list_of_numbers = []\n",
    "for i in [2, 4, 6]:\n",
    "    list_of_numbers.append(i ** 2)\n",
    "    \n",
    "# putting a variable at the end of a cell will print out the variable\n",
    "list_of_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 36]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do the same thing like this, which is called a list comprehension\n",
    "[i ** 2 for i in [2, 4, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "Now for the assignment.  Fill in all the blanks (three underscores, \\_\\_\\_) to get the code working.  You can use the test at the bottom of the file to make sure you've got it working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# built-in Python libraries\n",
    "import string\n",
    "\n",
    "# you probably need to download stopwords first:\n",
    "# https://stackoverflow.com/a/41640852/4549682\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')  # might have to use 'en' instead of 'english'\n",
    "stopwords = set(en_stopwords)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import functions from NLTK library\n",
    "\n",
    "from nltk.tokenize import word_tokenize  #split text into tokens by words and punctutation\n",
    "from nltk.probability import FreqDist    #frequency count for tokens \n",
    "from nltk.util import ngrams             #get n-connected tokens\n",
    "from nltk.stem import WordNetLemmatizer  #lemmatize words to dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatize function is set to false by default; pass True to lemmatize words\n",
    "#parts of speech arguments that can be passed:\n",
    "#ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "#parts of speech tag references sourced from: \n",
    "#https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "def clean_text(document, lemmatize=False, pos=None): \n",
    "    \"\"\"\n",
    "    cleans text for analysis\n",
    "    \n",
    "    - lowercases\n",
    "    - removes punctuation and numbers\n",
    "    - lemmatizes or stems\n",
    "    - removes stopwords\n",
    "    \"\"\"\n",
    "    # lowercase text\n",
    "    print('lowercasing')\n",
    "    # use the .lower method of strings to lowercase the document\n",
    "    # https://docs.python.org/3/library/stdtypes.html#str.lower\n",
    "    lowercased_doc = document.lower()\n",
    "    \n",
    "    print('removing punctuation/numbers')\n",
    "    # remove punctuation and numbers using the \"String constants\" from the string library:\n",
    "    # https://docs.python.org/3/library/string.html#string-constants\n",
    "    # do this before stemming, so things like \"act's\" turn into 'act' instead of 'act s'\n",
    "    table = str.maketrans({key: None for key in string.punctuation + string.digits + \"‘\" + \"’\"})\n",
    "    \n",
    "    # use the 'translate' method on each of the docs to remove punctuation\n",
    "    # here is an example: https://stackoverflow.com/a/34294398/4549682\n",
    "    clean_document = lowercased_doc.translate(table)\n",
    "    \n",
    "    # stem words -- basically chop off the ends\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    #printed_status=False\n",
    "    \n",
    "    stems = []\n",
    "    \n",
    "    # use the split method of strings to split the document (at spaces) into words:\n",
    "    # https://docs.python.org/3/library/stdtypes.html#str.split\n",
    "    # this will also remove extra spaces at the ends and beginnings of words\n",
    "    words = clean_document.split()\n",
    "    \n",
    "    #will print the status of either doing lemmitization or stemming\n",
    "    #based on argument passed in clean_text\n",
    "    if lemmatize==True: print(\"lemmatizing\")\n",
    "    else: print(\"stemming\")\n",
    "    \n",
    "    for w in words:\n",
    "        \n",
    "        if lemmatize==True:\n",
    "            if pos != None:\n",
    "            #lemmatize using part of speech passed in function argument\n",
    "                stems.append(wnl.lemmatize(w, pos=pos))\n",
    "            \n",
    "            else: #default part of speech is noun\n",
    "                stems.append(wnl.lemmatize(w))\n",
    "            \n",
    "        # stem the word with the stemmer, and add to the 'stems' list:\n",
    "        # http://www.nltk.org/howto/stem.html\n",
    "        else:\n",
    "            stems.append(stemmer.stem(w))\n",
    "\n",
    "    clean_document = stems\n",
    "    \n",
    "    # remove stopwords\n",
    "    print('removing stopwords')\n",
    "    # this is the list comprehension way to do it\n",
    "    # clean_document = [w for w in clean_document if w not in stopwords]\n",
    "    \n",
    "    clean_words = []\n",
    "    for w in clean_document:\n",
    "        # make sure the word is not in the stopword set\n",
    "        # remember -- we created the 'stopwords' variable in the cell above\n",
    "        if w not in stopwords:\n",
    "            clean_words.append(w)\n",
    "    \n",
    "    \"\"\"\n",
    "    join tokens back into a single string with the .join() method of strings:\n",
    "    https://www.tutorialspoint.com/python/string_join.htm\n",
    "    https://docs.python.org/3/library/stdtypes.html#str.join\n",
    "    You will want to join the strings with a single space.\n",
    "    \"\"\"\n",
    "    clean_document = \" \".join(clean_words)\n",
    "    \n",
    "    # return the clean document\n",
    "    return clean_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_string = 'this test string has    CAPS, numbers 9-8, punctuation !*&$, and stopwords like \"the\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercasing\n",
      "removing punctuation/numbers\n",
      "stemming\n",
      "removing stopwords\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hooray! You got it.\n"
     ]
    }
   ],
   "source": [
    "correct_output = 'thi test string ha cap number punctuat stopword like'\n",
    "your_result = clean_text(test_string)\n",
    "print('\\n'*3)\n",
    "\n",
    "if your_result == correct_output:\n",
    "    print('Hooray! You got it.')\n",
    "else:\n",
    "    print('Whoops, something is wrong.  Your function returned:')\n",
    "    print(your_result)\n",
    "    print('but it should look like:')\n",
    "    print(correct_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for the rest of the assignment, test it out on a guterberg book, a news article, or some other text.  Then get the top-frequency words with sklearn's CountVectorizer or nltk's FreqDist, or Counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "Using functions from the Natural Language Toolkit library for Python, I read in the text from a file that has a [Grimms' Fairy Tales](http://www.gutenberg.org/ebooks/2591) excerpt of the story \"The Twelve Dancing Princessess\", written by Jacob Grimm and Wilhelm Grimm. The text read in as a string and was stored in a variable called `text`. In the `clean_text` function above, I modified the argument parameters to initialize a default variable `lemmatize` that is set to `False`. Further in the function with the list of clean words returned from the `.split` function (with the text lowercased and the punctuation/digits removed), the lemmatize Boolean argument determines if the list of words is lemmatized or stemmed. If an argument of `lemmatize=True` is passed into the `clean_text` function, then `lemmatizing` will be printed and the words will be reduced to their dictionary form based on the verb part of speech. Otherwise if there is no input given for the argument `lemmatize`, then the default value `lemmatize=False` remains and the word `stemming` will print before the `else` statement for the `for w in words` loop is initialized. In the `else` statement because the value of `lemmatize` is `False`, the `if lemmatize=True` statement will not trigger and instead the words will be stemmed.\n",
    "\n",
    "In addition to the `lemmatize` argument, I added a `pos` argument that would hold the part-of-speech argument that would determine which part of speech the text data should be lemmatized with. The default argument value is `None` so that in the function when no `pos` argument is given, the default value of noun (`\"n\"`) would be used with the WordNetLemmatizer function. \n",
    "\n",
    "Lastly, I added two non-standard apostrophe punctuation symbols to the `maketrans` function in order to also remove them from the string when cleaning the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in text from file as a string\n",
    "#string value is set to variable \"text\"\n",
    "with open(\"12dancingprincesses.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE T'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify the first 5 characters in the \"text\" string variable\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmed Tokens\n",
    "\n",
    "In this section of the code, I ran the `clean_text` function on the string text read in from the `12dancingprincesses.txt` file. Because I did not pass a `lemmatize=True` argument, the default argument `False` stayed as it was and my text was stemmed after removing punctuation and digits. Once the cleaned text returned, it was stored into a variable `clean_result` and passed to the NLTK `work_tokenize` function. Then the frequency for each tokenized words was calculated using the `FreqDist` function and the `most_common(10)` function returned the top 10 most frequently occuring words in the frequency distribution list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercasing\n",
      "removing punctuation/numbers\n",
      "stemming\n",
      "removing stopwords\n"
     ]
    }
   ],
   "source": [
    "#run the clean_text function on the \"text\" variable\n",
    "#default argument is to stem words (lemmatize=False)\n",
    "#words in this text will be stemmed\n",
    "\n",
    "#clean_result is a string\n",
    "clean_result = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separate words and leftover punctuation into tokens\n",
    "#this will generate a list\n",
    "tknzwords = word_tokenize(clean_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wa', 24),\n",
       " ('princess', 22),\n",
       " ('soldier', 19),\n",
       " ('king', 17),\n",
       " ('said', 16),\n",
       " ('danc', 12),\n",
       " ('hi', 12),\n",
       " ('twelv', 11),\n",
       " ('went', 11),\n",
       " ('came', 10)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use list of tokens\n",
    "#FreqDist will count the frequency of each token\n",
    "#most_common(10) will return top 10 frequently occuring tokens\n",
    "FreqDist(tknzwords).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatized Tokens\n",
    "\n",
    "For this section of code, I used the `clean_text` function but instead of using the default argument `lemmatize=False`, I passed in the `True` value to trigger the cleaned text (lowercased with removed punctuation and digits) to be lemmitized. The default argument for part-of-speech (pos) lemmtization is noun, however I have a second example using the argument `pos=\"v\"` to lemmatize words using the verb part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercasing\n",
      "removing punctuation/numbers\n",
      "lemmatizing\n",
      "removing stopwords\n"
     ]
    }
   ],
   "source": [
    "#use the clean_text function with True value for lemmatize argument\n",
    "lem_clean = clean_text(text, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn cleaned text into word tokens list\n",
    "tknzlem = word_tokenize(lem_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wa', 24),\n",
       " ('princess', 22),\n",
       " ('soldier', 19),\n",
       " ('king', 17),\n",
       " ('said', 16),\n",
       " ('twelve', 11),\n",
       " ('went', 11),\n",
       " ('came', 10),\n",
       " ('eldest', 10),\n",
       " ('bed', 8)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 frequently occuring words in the lemmatized word tokens list\n",
    "FreqDist(tknzlem).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercasing\n",
      "removing punctuation/numbers\n",
      "lemmatizing\n",
      "removing stopwords\n"
     ]
    }
   ],
   "source": [
    "#use clean_text function to lemmatize using verb part-of-speech\n",
    "vlem_clean = clean_text(text, lemmatize=True, pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word tokens list of verb-lemmatized text\n",
    "tknzvlem = word_tokenize(vlem_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soldier', 19),\n",
       " ('princesses', 17),\n",
       " ('say', 17),\n",
       " ('go', 16),\n",
       " ('king', 13),\n",
       " ('dance', 12),\n",
       " ('twelve', 11),\n",
       " ('come', 11),\n",
       " ('eldest', 10),\n",
       " ('bed', 8)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 frequently occuring words in verb-lemmatized list\n",
    "FreqDist(tknzvlem).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "Using the output from the Stemmed Tokens section, I used `for` loops to iterate through the items in the tokens lists and generated bigram and trigram lists in order to calculate the most frequently occuring n-grams (for n=2 and 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#empty list to hold bigram pair items\n",
    "bgs = []\n",
    "\n",
    "#add each bigram pair to bgs list\n",
    "for bigram in ngrams(tknzwords, 2):\n",
    "    bgs.append(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('princess', 'danc'), 4),\n",
       " (('went', 'bed'), 3),\n",
       " (('danc', 'night'), 3),\n",
       " (('king', 'son'), 3),\n",
       " (('came', 'wa'), 3),\n",
       " (('one', 'princess'), 3),\n",
       " (('third', 'night'), 3),\n",
       " (('said', 'wa'), 3),\n",
       " (('soldier', 'said'), 3),\n",
       " (('golden', 'cup'), 3)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 frequently occuring bigrams\n",
    "FreqDist(bgs).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#empty list to hold trigrams\n",
    "tgs = []\n",
    "\n",
    "#add each trigram set to list\n",
    "for trigram in ngrams(tknzwords, 3):\n",
    "    tgs.append(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('king', 'son', 'soon'), 2),\n",
       " (('second', 'third', 'night'), 2),\n",
       " (('princess', 'danc', 'time'), 2),\n",
       " (('take', 'care', 'drink'), 2),\n",
       " (('grove', 'tree', 'leav'), 2),\n",
       " (('three', 'branch', 'golden'), 2),\n",
       " (('branch', 'golden', 'cup'), 2),\n",
       " (('twelv', 'danc', 'princess'), 1),\n",
       " (('danc', 'princess', 'wa'), 1),\n",
       " (('princess', 'wa', 'king'), 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 frequently occuring trigrams\n",
    "FreqDist(tgs).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Overall, the `clean_text` function produced mostly similar output but with slight variations depending on the additional arguments passed into it. Words such as king, twelve, eldest, and princess were fairly among the top 10 frequently occuring words in the text. Both the stemmed and lemmatized (by noun) tokenized words returned \"wa\" as the most frequently occuring word. But since \"wa\" is not a recognized word in the English language, I did a manual search through the text and the only similar word that occurs so commonly would be \"was\", though I would need to research more as to how the functions from the `clean_text` function clean/tokenized the word in that way.\n",
    "\n",
    "In the stemmed list, the word \"went\" is in the top 10 of frequently occuring tokens. However, in the 10 most common tokens for the verb-lemmatized words, its dictionary form \"go\" made the top 10 in a slightly higher frequency (adding naturally occuring \"go\" word in the text to the count of lemmatized \"went\"). \n",
    "\n",
    "Comparing the bigrams output to the list of trigrams, some of the seemingly related n-grams were: \n",
    "\n",
    "`('princess', 'danc') -> ('princess', 'danc', 'time')` <br>\n",
    "`('king', 'son') -> ('king', 'son', 'soon')` <br>\n",
    "`('third', 'night') -> ('second', 'third', 'night')`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
